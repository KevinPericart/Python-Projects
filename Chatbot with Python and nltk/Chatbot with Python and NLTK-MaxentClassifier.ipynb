{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Chatbot with Python and NLTK and MaxentClassifier <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aims to build a chatbot with NLTK library and MaxentClassifier linked the crypto-currency domain.\n",
    "\n",
    "<b> Ressources <b> : https://medium.com/x8-the-ai-community/build-your-first-chatbot-in-python-334247814900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the Chatbot with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\macbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\macbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\macbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\macbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords', quiet = True, raise_on_error = True)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('nps_chat')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (\"hi\", \"hello\", \"hi there\")\n",
    "outputs = [\"hi\", \"hey\", \"hi there\", \"welcome\"]\n",
    "filename = \"source.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "remove_punctuation = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the format to better process classification\n",
    "def fetch_features(chat) : \n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(chat) :\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(tokens) : \n",
    "    return [lem.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text) :\n",
    "    return lemmatise(nltk.word_tokenize(text.lower().translate(remove_punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(sentence) :\n",
    "    for word in sentence.split() : \n",
    "        if word.lower() in inputs : \n",
    "            return random.choice(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(user_response) : \n",
    "    resp = ''\n",
    "    q_list.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = tokenise, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(q_list)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0) :\n",
    "        resp = resp + \"Sorry! I don't know the answer to this. Would you like to try again? Type bye to exit\"\n",
    "        return resp\n",
    "    else : \n",
    "        resp_ids = qa_dict[idx]\n",
    "        resp_str = ''\n",
    "        s_id = resp_ids[0]\n",
    "        end = resp_ids[1]\n",
    "        while s_id < end : \n",
    "            resp_str = resp_str + \" \" + sent_tokens[s_id]\n",
    "            s_id += 1\n",
    "        resp = resp + resp_str\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.050\n",
      "             2          -1.23669        0.851\n",
      "             3          -0.90765        0.884\n",
      "             4          -0.73889        0.900\n",
      "             5          -0.62801        0.911\n",
      "             6          -0.54673        0.921\n",
      "             7          -0.48390        0.926\n",
      "             8          -0.43420        0.932\n",
      "             9          -0.39454        0.936\n",
      "            10          -0.36269        0.940\n",
      "            11          -0.33682        0.944\n",
      "            12          -0.31547        0.947\n",
      "            13          -0.29753        0.949\n",
      "            14          -0.28218        0.951\n",
      "            15          -0.26887        0.953\n",
      "            16          -0.25717        0.955\n",
      "            17          -0.24678        0.956\n",
      "            18          -0.23748        0.957\n",
      "            19          -0.22909        0.958\n",
      "            20          -0.22147        0.959\n",
      "            21          -0.21451        0.960\n",
      "            22          -0.20812        0.961\n",
      "            23          -0.20223        0.962\n",
      "            24          -0.19678        0.962\n",
      "            25          -0.19172        0.963\n",
      "            26          -0.18700        0.964\n",
      "            27          -0.18258        0.964\n",
      "            28          -0.17844        0.965\n",
      "            29          -0.17455        0.966\n",
      "            30          -0.17089        0.966\n",
      "            31          -0.16743        0.967\n",
      "            32          -0.16416        0.967\n",
      "            33          -0.16106        0.967\n",
      "            34          -0.15811        0.967\n",
      "            35          -0.15531        0.968\n",
      "            36          -0.15265        0.969\n",
      "            37          -0.15010        0.969\n",
      "            38          -0.14767        0.970\n",
      "            39          -0.14535        0.970\n",
      "            40          -0.14313        0.970\n",
      "            41          -0.14100        0.971\n",
      "            42          -0.13895        0.971\n",
      "            43          -0.13699        0.971\n",
      "            44          -0.13510        0.972\n",
      "            45          -0.13328        0.972\n",
      "            46          -0.13153        0.972\n",
      "            47          -0.12984        0.973\n",
      "            48          -0.12822        0.973\n",
      "            49          -0.12664        0.973\n",
      "            50          -0.12513        0.973\n",
      "            51          -0.12366        0.974\n",
      "            52          -0.12224        0.974\n",
      "            53          -0.12086        0.974\n",
      "            54          -0.11953        0.974\n",
      "            55          -0.11824        0.975\n",
      "            56          -0.11699        0.975\n",
      "            57          -0.11577        0.975\n",
      "            58          -0.11459        0.975\n",
      "            59          -0.11344        0.976\n",
      "            60          -0.11233        0.976\n",
      "            61          -0.11124        0.976\n",
      "            62          -0.11019        0.976\n",
      "            63          -0.10916        0.976\n",
      "            64          -0.10816        0.976\n",
      "            65          -0.10719        0.977\n",
      "            66          -0.10624        0.977\n",
      "            67          -0.10531        0.977\n",
      "            68          -0.10441        0.978\n",
      "            69          -0.10353        0.978\n",
      "            70          -0.10267        0.978\n",
      "            71          -0.10183        0.978\n",
      "            72          -0.10101        0.978\n",
      "            73          -0.10021        0.978\n",
      "            74          -0.09943        0.978\n",
      "            75          -0.09866        0.978\n",
      "            76          -0.09791        0.979\n",
      "            77          -0.09718        0.979\n",
      "            78          -0.09646        0.979\n",
      "            79          -0.09576        0.979\n",
      "            80          -0.09507        0.979\n",
      "            81          -0.09440        0.979\n",
      "            82          -0.09374        0.979\n",
      "            83          -0.09309        0.979\n",
      "            84          -0.09246        0.979\n",
      "            85          -0.09184        0.979\n",
      "            86          -0.09123        0.979\n",
      "            87          -0.09063        0.979\n",
      "            88          -0.09005        0.979\n",
      "            89          -0.08947        0.979\n",
      "            90          -0.08891        0.979\n",
      "            91          -0.08835        0.979\n",
      "            92          -0.08781        0.980\n",
      "            93          -0.08728        0.980\n",
      "            94          -0.08675        0.980\n",
      "            95          -0.08624        0.980\n",
      "            96          -0.08573        0.980\n",
      "            97          -0.08523        0.980\n",
      "            98          -0.08474        0.980\n",
      "            99          -0.08426        0.980\n",
      "         Final          -0.08379        0.980\n"
     ]
    }
   ],
   "source": [
    "chats = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "featuresets = [(fetch_features(chat.text), chat.get('class')) for chat in chats]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_bank = open(filename, 'r', errors = 'ignore')\n",
    "qb_text = ques_bank.read()\n",
    "qb_text = qb_text.lower()\n",
    "sent_tokens = nltk.sent_tokenize(qb_text)\n",
    "word_tokens = nltk.word_tokenize(qb_text)\n",
    "qa_dict = {}\n",
    "q_list = []\n",
    "s_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "while s_count < len(sent_tokens) : \n",
    "    result = classifier.classify(fetch_features(sent_tokens[s_count]))\n",
    "    if(\"question\" in result.lower()) : \n",
    "        next_question_id = s_count + 1\n",
    "        next_question = classifier.classify(fetch_features(sent_tokens[next_question_id]))\n",
    "        while(not(\"question\" in next_question.lower()) and next_question_id < len(sent_tokens) - 1) : \n",
    "            next_question_id += 1\n",
    "            next_question = classifier.classify(fetch_features(sent_tokens[next_question_id]))\n",
    "        q_list.append(sent_tokens[s_count])\n",
    "        end = next_question_id\n",
    "        if(next_question_id - s_count > 5) : \n",
    "            end = s_count + 5\n",
    "        qa_dict.update({len(q_list) - 1:[s_count + 1, end]})\n",
    "        s_count = next_question_id\n",
    "    else : \n",
    "        s_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mC-3PO :\n",
      "I am C-3PO, I have all the answers If you want to exit, type bye\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "bye\n",
      "\u001b[1m\u001b[34m\n",
      "C-3PO : Bye! Take care\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(colored(\"C-3PO :\\nI am C-3PO, I have all the answers If you want to exit, type bye\", \"blue\"))\n",
    "while(flag == True) : \n",
    "    print(colored(\"\\nYOU :\", 'red', attrs = ['bold']))\n",
    "    u_input = input()\n",
    "    u_input = u_input.lower()\n",
    "    if(u_input != 'bye') : \n",
    "        if(greet(u_input) != None) : \n",
    "            print(colored(\"\\nC-3PO :\", 'blue', attrs = ['bold']))\n",
    "            print(greet(u_input))\n",
    "        else :\n",
    "            print(colored(\"\\nC-3PO :\", 'blue', attrs = ['bold']))\n",
    "            print(colored(match(u_input).strip().capitalize(), 'blue'))\n",
    "            q_list.remove(u_input)\n",
    "    else : \n",
    "        flag = False\n",
    "        print(colored(\"\\nC-3PO : Bye! Take care\", 'blue', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.050\n",
      "             2          -1.23669        0.851\n",
      "             3          -0.90765        0.884\n",
      "             4          -0.73889        0.900\n",
      "             5          -0.62801        0.911\n",
      "             6          -0.54673        0.921\n",
      "             7          -0.48390        0.926\n",
      "             8          -0.43420        0.932\n",
      "             9          -0.39454        0.936\n",
      "            10          -0.36269        0.940\n",
      "            11          -0.33682        0.944\n",
      "            12          -0.31547        0.947\n",
      "            13          -0.29753        0.949\n",
      "            14          -0.28218        0.951\n",
      "            15          -0.26887        0.953\n",
      "            16          -0.25717        0.955\n",
      "            17          -0.24678        0.956\n",
      "            18          -0.23748        0.957\n",
      "            19          -0.22909        0.958\n",
      "            20          -0.22147        0.959\n",
      "            21          -0.21451        0.960\n",
      "            22          -0.20812        0.961\n",
      "            23          -0.20223        0.962\n",
      "            24          -0.19678        0.962\n",
      "            25          -0.19172        0.963\n",
      "            26          -0.18700        0.964\n",
      "            27          -0.18258        0.964\n",
      "            28          -0.17844        0.965\n",
      "            29          -0.17455        0.966\n",
      "            30          -0.17089        0.966\n",
      "            31          -0.16743        0.967\n",
      "            32          -0.16416        0.967\n",
      "            33          -0.16106        0.967\n",
      "            34          -0.15811        0.967\n",
      "            35          -0.15531        0.968\n",
      "            36          -0.15265        0.969\n",
      "            37          -0.15010        0.969\n",
      "            38          -0.14767        0.970\n",
      "            39          -0.14535        0.970\n",
      "            40          -0.14313        0.970\n",
      "            41          -0.14100        0.971\n",
      "            42          -0.13895        0.971\n",
      "            43          -0.13699        0.971\n",
      "            44          -0.13510        0.972\n",
      "            45          -0.13328        0.972\n",
      "            46          -0.13153        0.972\n",
      "            47          -0.12984        0.973\n",
      "            48          -0.12822        0.973\n",
      "            49          -0.12664        0.973\n",
      "            50          -0.12513        0.973\n",
      "            51          -0.12366        0.974\n",
      "            52          -0.12224        0.974\n",
      "            53          -0.12086        0.974\n",
      "            54          -0.11953        0.974\n",
      "            55          -0.11824        0.975\n",
      "            56          -0.11699        0.975\n",
      "            57          -0.11577        0.975\n",
      "            58          -0.11459        0.975\n",
      "            59          -0.11344        0.976\n",
      "            60          -0.11233        0.976\n",
      "            61          -0.11124        0.976\n",
      "            62          -0.11019        0.976\n",
      "            63          -0.10916        0.976\n",
      "            64          -0.10816        0.976\n",
      "            65          -0.10719        0.977\n",
      "            66          -0.10624        0.977\n",
      "            67          -0.10531        0.977\n",
      "            68          -0.10441        0.978\n",
      "            69          -0.10353        0.978\n",
      "            70          -0.10267        0.978\n",
      "            71          -0.10183        0.978\n",
      "            72          -0.10101        0.978\n",
      "            73          -0.10021        0.978\n",
      "            74          -0.09943        0.978\n",
      "            75          -0.09866        0.978\n",
      "            76          -0.09791        0.979\n",
      "            77          -0.09718        0.979\n",
      "            78          -0.09646        0.979\n",
      "            79          -0.09576        0.979\n",
      "            80          -0.09507        0.979\n",
      "            81          -0.09440        0.979\n",
      "            82          -0.09374        0.979\n",
      "            83          -0.09309        0.979\n",
      "            84          -0.09246        0.979\n",
      "            85          -0.09184        0.979\n",
      "            86          -0.09123        0.979\n",
      "            87          -0.09063        0.979\n",
      "            88          -0.09005        0.979\n",
      "            89          -0.08947        0.979\n",
      "            90          -0.08891        0.979\n",
      "            91          -0.08835        0.979\n",
      "            92          -0.08781        0.980\n",
      "            93          -0.08728        0.980\n",
      "            94          -0.08675        0.980\n",
      "            95          -0.08624        0.980\n",
      "            96          -0.08573        0.980\n",
      "            97          -0.08523        0.980\n",
      "            98          -0.08474        0.980\n",
      "            99          -0.08426        0.980\n",
      "         Final          -0.08379        0.980\n",
      "0.712\n"
     ]
    }
   ],
   "source": [
    "chats = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "featuresets = [(fetch_features(chat.text), chat.get('class')) for chat in chats]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART\n",
      "sho*\n",
      ".ACTION keeps 10-19-20sUser115s place nice and warm.\n",
      "hey any guys with cams wanna play?\n",
      ".ACTION sits on 10-19-20sUser68's lap.\n",
      "JOIN\n",
      "JOIN\n",
      "any guyz wanna chat \n",
      "hi there\n",
      "boo, it's a female.\n",
      "hey 10-19-20sUser126\n",
      "PART\n",
      "i wonna chat\n",
      "PART\n",
      "24 f nc single mom\n",
      "where did everyone gooo?\n",
      "sure 10-19-20sUser126\n",
      "JOIN\n",
      "what did you but on e-bay\n",
      "i feel like im in the wrong room\n",
      "yeee haw 10-19-20sUser30\n",
      "im considering changing my nickname to \"ihavehotnips\"\n",
      "JOIN\n",
      "i don't want hot pics of a female, I can look in a mirror.\n",
      "hi 10-19-20sUser64\n",
      "wb 10-19-20sUser139\n",
      "u should 10-19-20sUser44:)\n",
      "PART\n",
      "PART\n",
      "JOIN\n",
      "single dad here\n",
      "JOIN\n",
      "ty 10-19-20sUser68\n",
      "PART\n",
      "JOIN\n",
      "Hi 10-19-20sUser139\n",
      "PART\n",
      "JOIN\n",
      "hi 10-19-20sUser138\n",
      "HAHAHA\n",
      "yw 10-19-20sUser139\n",
      "you should make it 'iamahotnip', 10-19-20sUser44\n",
      "alright\n",
      "hi 10-19-20sUser139.\n",
      "you're fucking hot.\n",
      "i thought of that!\n",
      "hi 10-19-20sUser126, its so late\n",
      "lmao\n",
      "ahah \"iamahotniplickme\"\n",
      "PART\n"
     ]
    }
   ],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "for post in posts[100:150] : \n",
    "    print(post.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.70805        0.050\n",
      "             2          -1.23669        0.851\n",
      "             3          -0.90765        0.884\n",
      "             4          -0.73889        0.900\n",
      "             5          -0.62801        0.911\n",
      "             6          -0.54673        0.921\n",
      "             7          -0.48390        0.926\n",
      "             8          -0.43420        0.932\n",
      "             9          -0.39454        0.936\n",
      "            10          -0.36269        0.940\n",
      "            11          -0.33682        0.944\n",
      "            12          -0.31547        0.947\n",
      "            13          -0.29753        0.949\n",
      "            14          -0.28218        0.951\n",
      "            15          -0.26887        0.953\n",
      "            16          -0.25717        0.955\n",
      "            17          -0.24678        0.956\n",
      "            18          -0.23748        0.957\n",
      "            19          -0.22909        0.958\n",
      "            20          -0.22147        0.959\n",
      "            21          -0.21451        0.960\n",
      "            22          -0.20812        0.961\n",
      "            23          -0.20223        0.962\n",
      "            24          -0.19678        0.962\n",
      "            25          -0.19172        0.963\n",
      "            26          -0.18700        0.964\n",
      "            27          -0.18258        0.964\n",
      "            28          -0.17844        0.965\n",
      "            29          -0.17455        0.966\n",
      "            30          -0.17089        0.966\n",
      "            31          -0.16743        0.967\n",
      "            32          -0.16416        0.967\n",
      "            33          -0.16106        0.967\n",
      "            34          -0.15811        0.967\n",
      "            35          -0.15531        0.968\n",
      "            36          -0.15265        0.969\n",
      "            37          -0.15010        0.969\n",
      "            38          -0.14767        0.970\n",
      "            39          -0.14535        0.970\n",
      "            40          -0.14313        0.970\n",
      "            41          -0.14100        0.971\n",
      "            42          -0.13895        0.971\n",
      "            43          -0.13699        0.971\n",
      "            44          -0.13510        0.972\n",
      "            45          -0.13328        0.972\n",
      "            46          -0.13153        0.972\n",
      "            47          -0.12984        0.973\n",
      "            48          -0.12822        0.973\n",
      "            49          -0.12664        0.973\n",
      "            50          -0.12513        0.973\n",
      "            51          -0.12366        0.974\n",
      "            52          -0.12224        0.974\n",
      "            53          -0.12086        0.974\n",
      "            54          -0.11953        0.974\n",
      "            55          -0.11824        0.975\n",
      "            56          -0.11699        0.975\n",
      "            57          -0.11577        0.975\n",
      "            58          -0.11459        0.975\n",
      "            59          -0.11344        0.976\n",
      "            60          -0.11233        0.976\n",
      "            61          -0.11124        0.976\n",
      "            62          -0.11019        0.976\n",
      "            63          -0.10916        0.976\n",
      "            64          -0.10816        0.976\n",
      "            65          -0.10719        0.977\n",
      "            66          -0.10624        0.977\n",
      "            67          -0.10531        0.977\n",
      "            68          -0.10441        0.978\n",
      "            69          -0.10353        0.978\n",
      "            70          -0.10267        0.978\n",
      "            71          -0.10183        0.978\n",
      "            72          -0.10101        0.978\n",
      "            73          -0.10021        0.978\n",
      "            74          -0.09943        0.978\n",
      "            75          -0.09866        0.978\n",
      "            76          -0.09791        0.979\n",
      "            77          -0.09718        0.979\n",
      "            78          -0.09646        0.979\n",
      "            79          -0.09576        0.979\n",
      "            80          -0.09507        0.979\n",
      "            81          -0.09440        0.979\n",
      "            82          -0.09374        0.979\n",
      "            83          -0.09309        0.979\n",
      "            84          -0.09246        0.979\n",
      "            85          -0.09184        0.979\n",
      "            86          -0.09123        0.979\n",
      "            87          -0.09063        0.979\n",
      "            88          -0.09005        0.979\n",
      "            89          -0.08947        0.979\n",
      "            90          -0.08891        0.979\n",
      "            91          -0.08835        0.979\n",
      "            92          -0.08781        0.980\n",
      "            93          -0.08728        0.980\n",
      "            94          -0.08675        0.980\n",
      "            95          -0.08624        0.980\n",
      "            96          -0.08573        0.980\n",
      "            97          -0.08523        0.980\n",
      "            98          -0.08474        0.980\n",
      "            99          -0.08426        0.980\n",
      "         Final          -0.08379        0.980\n",
      "0.712\n"
     ]
    }
   ],
   "source": [
    "chats = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "def fetch_features(chat) : \n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(chat) :\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features\n",
    "featuresets = [(fetch_features(chat.text), chat.get('class')) for chat in chats]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a question bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_bank = open('crypto_faq.txt','r',errors = 'ignore')\n",
    "qb_text = ques_bank.read()\n",
    "qb_text = qb_text.lower()\n",
    "sent_tokens = nltk.sent_tokenize(qb_text)\n",
    "word_tokens = nltk.word_tokenize(qb_text)\n",
    "qa_dict = {}\n",
    "q_list = [] \n",
    "s_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing and fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "while s_count < len(sent_tokens):\n",
    "    result = classifier.classify(fetch_features(sent_tokens[s_count]))\n",
    "    if(\"question\" in result.lower()) :\n",
    "        next_question_id = s_count + 1\n",
    "        next_question = classifier.classify(fetch_features(sent_tokens[next_question_id]))\n",
    "        while(not(\"question\" in next_question.lower()) and next_question_id < len(sent_tokens) - 1) :\n",
    "            next_question_id += 1\n",
    "            next_question = classifier.classify(fetch_features(sent_tokens[next_question_id]))\n",
    "        q_list.append(sent_tokens[s_count])\n",
    "        end = next_question_id\n",
    "        if(next_question_id - s_count > 5) :\n",
    "            end = s_count + 5\n",
    "        qa_dict.update({len(q_list) - 1:[s_count + 1,end]})\n",
    "        s_count = next_question_id\n",
    "    else:\n",
    "        s_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(tokens):\n",
    "    return [lem.lemmatize(token) for token in tokens]\n",
    "remove_punctuation = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    return lemmatise(nltk.word_tokenize(text.lower().translate(remove_punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(user_response):\n",
    "    resp = ''\n",
    "    q_list.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = tokenise, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(q_list)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        resp = resp + \"Sorry! I don't know the answer to this. Would you like to try again. Type bye to exit\"\n",
    "        return resp\n",
    "    else:\n",
    "        resp_ids = qa_dict[idx]\n",
    "        resp_str = ''\n",
    "        s_id = resp_ids[0]\n",
    "        end = resp_ids[1]\n",
    "        while s_id < end :\n",
    "            resp_str = resp_str + \" \" + sent_tokens[s_id]\n",
    "            s_id += 1\n",
    "        resp = resp + resp_str\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mC-3PO :\n",
      "I am C-3PO, I have all the answers If you want to exit, type bye\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "hi\n",
      "\u001b[1m\u001b[34m\n",
      "C-3PO :\u001b[0m\n",
      "hi there\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "bitcoin\n",
      "\u001b[1m\u001b[34m\n",
      "C-3PO :\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macbi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mThe bitcoin protocol can change the financial landscape we see today. the protocol can act as a currency, voting mechanism, global identification and reputation application, a micro-tipper, crowdfunding platform, initiate trusts, wills and contracts, decentralized domain names, future markets, and basically everything the financial system of today can handle plus so much more. the currency application is just the beginning of this evolution of world's finances. what happens if i lose my bitcoins?\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "ethereum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macbi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "C-3PO :\u001b[0m\n",
      "\u001b[34mEthereum is a decentralized smart contracts platform that is powered by a cryptocurrency called ether. a good starting point to learn more about its workings would be the â€œwhat is ethereum?â€ page.\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "cryptocurrency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macbi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "C-3PO :\u001b[0m\n",
      "\u001b[34mA3. cryptocurrency is a type of virtual currency that uses cryptography to secure transactions that are digitally recorded on a distributed ledger, such as a blockchain. a transaction involving cryptocurrency that is recorded on a distributed ledger is referred to as an â€œon-chainâ€ transaction; a transaction that is not recorded on the distributed ledger is referred to as an â€œoff-chainâ€ transaction. q4.\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "tax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macbi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "C-3PO :\u001b[0m\n",
      "\u001b[34mA44. information on virtual currency is available at virtual currencies (irs.gov/virtual_currency). many questions about the tax treatment of virtual currency can be answered by referring to notice 2014-21 pdf and rev. rul.\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n",
      "virtual currency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macbi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "C-3PO :\u001b[0m\n",
      "\u001b[34mA1. virtual currency is a digital representation of value, other than a representation of the u.s. dollar or a foreign currency (â€œreal currencyâ€), that functions as a unit of account, a store of value, and a medium of exchange. some virtual currencies are convertible, which means that they have an equivalent value in real currency or act as a substitute for real currency. the irs uses the term â€œvirtual currencyâ€ in these faqs to describe the various types of convertible virtual currency that are used as a medium of exchange, such as digital currency and cryptocurrency.\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "YOU :\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(colored(\"C-3PO :\\nI am C-3PO, I have all the answers If you want to exit, type bye\", 'blue', attrs = ['bold']))\n",
    "while(flag == True) : \n",
    "    print(colored(\"\\nYOU :\", 'red', attrs = ['bold']))\n",
    "    u_input = input()\n",
    "    u_input = u_input.lower()\n",
    "    if(u_input != 'bye') : \n",
    "        if(greet(u_input) != None) : \n",
    "            print(colored(\"\\nC-3PO :\", 'blue', attrs = ['bold']))\n",
    "            print(greet(u_input))\n",
    "        else :\n",
    "            print(colored(\"\\nC-3PO :\", 'blue', attrs = ['bold']))\n",
    "            print(colored(match(u_input).strip().capitalize(), 'blue'))\n",
    "            q_list.remove(u_input)\n",
    "    else : \n",
    "        flag = False\n",
    "        print(colored(\"\\nC-3PO : Bye! Take care\", 'blue', attrs = ['bold']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
